{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5ba0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d02241",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 \n",
    "jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.shine.com/ \n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the  location” field. \n",
    "3. Then click the search button. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa68cf57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analyst Jobs:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_jobs(job_title, location, num_jobs=10):\n",
    "    # Step 1: Get the webpage\n",
    "    base_url = \"https://www.shine.com/\"\n",
    "    search_url = f\"{base_url}job-search/{job_title}-jobs-in-{location}\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 2-4: Scrape data for the first 10 jobs\n",
    "    jobs_data = []\n",
    "    job_elements = soup.find_all('div', class_='joblisting')\n",
    "    for job_element in job_elements[:num_jobs]:\n",
    "        # Extract job details\n",
    "        job_title = job_element.find('h2', class_='job_title').text.strip()\n",
    "        job_location = job_element.find('span', class_='job_location').text.strip()\n",
    "        company_name = job_element.find('span', class_='comp_name').text.strip()\n",
    "        experience_required = job_element.find('span', class_='exp').text.strip()\n",
    "\n",
    "        # Store the extracted data in a dictionary\n",
    "        jobs_data.append({\n",
    "            'Job Title': job_title,\n",
    "            'Job Location': job_location,\n",
    "            'Company Name': company_name,\n",
    "            'Experience Required': experience_required\n",
    "        })\n",
    "\n",
    "    # Step 5: Create a dataframe\n",
    "    df = pd.DataFrame(jobs_data)\n",
    "    return df\n",
    "\n",
    "# Q1\n",
    "data_analyst_df = scrape_jobs(\"Data Analyst\", \"Bangalore\")\n",
    "print(\"Data Analyst Jobs:\")\n",
    "print(data_analyst_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93006d",
   "metadata": {},
   "source": [
    "2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You \n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.shine.com/ \n",
    "2.  Enter “Data Scientist” in “Job title, Skills”  field and enter “Bangalore” in “enter the location” field. \n",
    "3. Then click the search  button. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary tools\n",
    "import requests  # Helps in making web requests\n",
    "from bs4 import BeautifulSoup  # Helps in reading web page content\n",
    "import pandas as pd  # Helps in working with data tables\n",
    "\n",
    "# Function to find Data Scientist jobs in Bangalore\n",
    "def find_data_scientist_jobs():\n",
    "    # Step 1: Open the Shine website\n",
    "    base_url = \"https://www.shine.com/\"\n",
    "    # Step 2: Look for Data Scientist jobs in Bangalore\n",
    "    search_url = f\"{base_url}job-search/Data-Scientist-jobs-in-Bangalore\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 3-4: Extract information from the first 10 job listings\n",
    "    jobs_data = []\n",
    "    job_elements = soup.find_all('div', class_='joblisting')\n",
    "    for job_element in job_elements[:10]:\n",
    "        # Extract job details\n",
    "        job_title = job_element.find('h2', class_='job_title').text.strip()\n",
    "        job_location = job_element.find('span', class_='job_location').text.strip()\n",
    "        company_name = job_element.find('span', class_='comp_name').text.strip()\n",
    "\n",
    "        # Store job details in a dictionary\n",
    "        job_info = {\n",
    "            'Job Title': job_title,\n",
    "            'Job Location': job_location,\n",
    "            'Company Name': company_name\n",
    "        }\n",
    "        jobs_data.append(job_info)\n",
    "\n",
    "    # Step 5: Create a table of job details\n",
    "    job_table = pd.DataFrame(jobs_data)\n",
    "    return job_table\n",
    "\n",
    "# Q2: Finding Data Scientist jobs in Bangalore\n",
    "data_scientist_jobs = find_data_scientist_jobs()\n",
    "print(\"Data Scientist Jobs in Bangalore:\")\n",
    "print(data_scientist_jobs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
